title: The third time taking an ML course

First time was CoderSchool's invited course at Fulright by Mr. Hai. Second time was Vincent Ng's visiting course at Fulbright. Third time is Vibhav Gogate's graduate level course at UTD.  That is not to mention Andrew Ng's Machine Learning course on Coursera that I took during undergrad, and MIT's Deep Learning video course that I took during that time too, but during a STEM Club's co-learning initiative.


It is safe to say that I don't need to formally take a machine learning course anymore.

But there is always a feelings of not knowing enough in machine learning.


My first time interacting with machine learning was in highschool, when my olympiad teacher showed me "machinelearningcoban.com" (in Vietnamese) and "machinelearningmastery.com". They were good sources, but not comprehensive. 

What has changed over the years?
- Back in 2016, RNN and LSTM was quite popular.
- In 2019, the instructor told me that XGBoost was the most powerful. He focused on the top-performers on Kaggle. Maybe he mentioned Yann Lecun?
- In 2020, I learned from Andrew Ng during Covid. It was informative. The things I remember the mosts are the practical advices he gave, such as where to improve in an ML pipeline. 
- In 2022, transformers were more abundant, ChatGPT started to become more popular. I was excited to learn NLP from Vincent Ng. But his ML course focused a lot on the math foundation of things.
- Now in 2024, there are a few more things that I apprecite learning, such as PAC model, more unifying perspectives of the algorithms. I know Bayes net more. I got to know that Tom Mitchell's ML textbook is so old and outdated. Also know that Kevin Murphy is a more updated book.

I suddenly realize that I have gone through the condition of an "ML insider" (trying my best to avoid the word "expert"). But actually, I do feel like I only know very little. There are things that I should know more about this field, including:
- The satistical side of machine learning, such as Monte Carlo Methods for inference?, MCMC, Gibbs Sampling, (Christian Robert is the main inspiration for this blog!).
- The path from traditional methods to transformers
- Other methods in the machine learning toolkit, and their foundations. Is there always a unifying perspective to view everything? Is the current methods just prompt engineering, or there's something deeper to it? Is Mixture Of Experts just a hyped name for ensemble? What has transformer fundamentally changed the problem of machine lnearing? (Or has it?) Why are people nowsadays tackle the classical problems (like regression, classification) with auto-regressive language-token-based LLMs only?

Learning about fundamental machine learning has a lot of benefits in this age. It helps put the GenAI hype into perspective. It gives more rigor into this experimental field. It helps you stand a bit more still in the storm of LLM hype.
