---
layout: post
title: GPT From Scratch
tags: cs
---

## Introduction

After [being motivated]({%post_url 2023-11-15-transformers-wikipedia%}) to learn about transformer in more depth, I spent a good chunk of time to implement the GPT model from scratch, without copying and pasting. And I was actually able to reproduce the same performance as the one in the tutorial by Andrej Karpathy. 

However, my goals are not just that. I wanted to find out the relationship between the hyperparameters and the complexity of the transformer, which is measured by (1) the number of trainable parameters and (2) the time complexity to generate one token. Therefore, during the subsequent sections, we will pay close attention to two things -- shape of the data, and shape of the trainable matrices.

This post is long, and should be used as a reference when reasoning about transformers. If you want new insights, skip to the last section -- I have some seemingly mind-blowing answers to the above questions!

*Huge thanks to Andrej Karpathy for the [tutorial](https://www.youtube.com/watch?v=kCc8FmEb1nY) on decoder-only transformer from scratch.*

## Preprocessing

First thing first, we are given a corpus, which is text. Therefore, we need two functions to convert between text and vectors:
- `encode(str) -> list[int]`
- `decode(list[int]) -> str or list[str]`

Input shapes:
- For training, each *training step* needs `xb` and `yb`, both with shape `(BATCH_SIZE, BLOCK_SIZE, VOCAB_SIZE)`
	- `yb` is the result by moving the window one step to the future from `xb`.
	- (There will be a way to prevent data leak from the future in the self-attention mechanism.)
- For inference (or text generation in particular)
	- start with a text -- can be a whole prompt or just a character (like `\n`)
	- encode it/them to a batch with basically the same general shape as above. But now batch size and block size depends on the given prompts.

Given this similarity in general structure, we will note the input shape the same moving forward.

## Model architecture (decoder-only)
### Hyperparams 

(with values from the tutorial -- probably matched with the 2017 Attention paper)
- `BATCH_SIZE` ($$B$$) = 64
- `BLOCK_SIZE` ($$T$$) = 256; context window length
- `VOCAB_SIZE` ($$C$$) = depends on the corpus
- `EMBED_SIZE` ($$E$$) = 384
- `N_BLOCKS` ($$L$$) = 6; num decoder blocks
- `N_HEADS` = 6; num attention heads inside a multi-head attention component (which is inside a decoder block)
- `DROPOUT` = 0.2; dropout probability

### Computations 
For training and 'primitive' inference, i.e., `forward()`
- Start with **input** of size `BxTxC`
- **Embedding**: sum of token embedding and positional embedding
	- **Token embedding**: simply a **trainable** matrix of size `CxE`. It acts as a lookup table for each token. Implementation-wise, the tokens can be represented as a matrix of one-hot vectors. That will be multiplied with this token embedding matrix.
	- **Positional embedding**: a fixed matrix of size `TxE`. It maps each position slot in the input sequence with an (ideally unique) number that contains the positional info.
	- **Output shape** is `BxTxE` -- the data is now officially in the *embedding space*!
- **Decoder** block x `N_BLOCKS`. Each Decoder block is laid out as follows:
	- A [LayerNorm](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html), with 2 **trainable** params ($$\gamma$$ and $$\beta$$).
	- **Multi-head Attention**: Attention head x `N_HEADS`. 
		- Each **Attention** head has:
			- `head_size = EMBED_SIZE // N_HEADS` ($$H$$) (related to the upcoming concatenation of output)
			- Key, query, and value **trainable** weight matrices, each has size `ExH`.
				- Each of these matrix transform the **data** into $$K$$, $$Q$$, $$V$$, of size `BxTxH`
				- Attention formula: $$\text{Attention}(K, Q, V) = \text{softmax}\left(\text{tril}_{t,t}\times\frac{Q\times K^T}{\sqrt{E}}\right).V$$
					- `tril` is a `t` $$\times$$ `t` (`t <= T`) lower triangular matrix to bring the averaged information of previous tokens to the next token
					- Softmax-ed data is actually dropped out before begin multiplied with $$V$$.
				- Data is merged from the three, still has shape `BxTxH`
			- This is where the tokens *talk* to each other, thus 'self-attention'.
		- Output of all Attention unit is concatenated along the 2nd (i.e., last) dimension. Because `H x N_HEADS = E`, **data shape** is now back to the good old `BxTxE`!
		- Then it is multiplied with a **trainable** `ExE` matrix called the *projection* layer. It is to prepare date for the skip connection later.
		- There is a dropout layer here (parametrized by `DROPOUT`).
		- There is also a skip connection, where input is added directly to the output. This is poetically called the *residual pathway*.
	- **FFNN**
		- A LayerNorm with 2 **trainable** params.
		- A Multi Layer Perceptron with two **trainable** matrices of size `Ex(4E)`, with an activation in the middle and a Dropout at the end.
		- Also has a skip connection.
		- This is where the tokens 'think for themselves'.
- A **language modeling** head (code name: `lm_head`)
	- A LayerNorm with 2 **trainable** params.
	- Then a **trainable** `ExC` matrix
	- **Data** is now mapped back to the 'language space'!

## Training loop
- Loss: cross entropy loss between the logits (`BxTxC`) with the actual next tokens (`BxT`)
- `AdamW` optimizer should be used. It seems to be the best.
- Doing eval once in a while by doing with-label inference on both train and val set. Then get the average loss. Remember to set `torch.no_grad()` and `model.eval()` before; and `model.train()` after.

## Inference
*(This is how ChatGPT actually spit out text!!!)* So we were left off at `forward()`, which returns a logits of shape `BxTxC`. From here, we do **auto-regressive** generation by repeatedly:
- first, obtaining a probability distribution from `logits` by softmax,
- then sampling a new token in the sequence (so `B` tokens for the whole batch).
- finally, from this new token, prepare the next batch of input ids to feed to `forward()` and get `logits` again
Do this until some stop condition, like `max_new_tokens` met. (Beam search may also be used?)

## Observations
I roughly estimated the complexity of this algorithm:
- Number of trainable params (proxy for model size) = $$O[E(B+C+E)]$$. 
    - See something? The model size seems to be independent from the context length?!! I originally did not believe this. I checked mine and Andrej's code -- the `block_size` variable is indeed not governing any matrix's size!
    - If this is true, why is there a context length limit on published models? I suspect that is just because the internal buffer for the *triangular matrix* (to prevent info flowing backwards) has a fixed size of $$T \times T$$. If we are allowed to change that, seems like any standard transformer can theoretically take arbitrarily long input! (It will just run for longer.)
- The (sequential) time complexity for predicting/training one token is roughly $$O[TCE+L(T^3+T^2H+TE^2)]$$.
    - Here we actually see $$T$$ plays a big role.
    - Given these variables (except $$L$$) are usually in the order of at least $$1000$$, the cost of predicting on token is currently about $$1B$$, times a constant (I guess about 20). Not a small cost!